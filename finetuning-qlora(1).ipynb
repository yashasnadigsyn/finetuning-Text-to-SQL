{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment Setup and Imports","metadata":{}},{"cell_type":"code","source":"!pip install -q -U transformers peft datasets accelerate trl bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:20:56.755974Z","iopub.execute_input":"2025-05-30T02:20:56.756637Z","iopub.status.idle":"2025-05-30T02:22:29.546407Z","shell.execute_reply.started":"2025-05-30T02:20:56.756612Z","shell.execute_reply":"2025-05-30T02:22:29.545671Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nimport pandas as pd\nfrom datasets import load_dataset\nfrom IPython.display import HTML, display\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline as hf_pipeline, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:22:29.547677Z","iopub.execute_input":"2025-05-30T02:22:29.547986Z","iopub.status.idle":"2025-05-30T02:22:54.730843Z","shell.execute_reply.started":"2025-05-30T02:22:29.547962Z","shell.execute_reply":"2025-05-30T02:22:54.730201Z"}},"outputs":[{"name":"stderr","text":"2025-05-30 02:22:40.849482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748571761.042459      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748571761.100796      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Dataset Preparation","metadata":{}},{"cell_type":"code","source":"# --- 2.1 Load Raw Dataset ---\ndataset_name = \"b-mc2/sql-create-context\"\ndataset = load_dataset(dataset_name, split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:22:54.731550Z","iopub.execute_input":"2025-05-30T02:22:54.732156Z","iopub.status.idle":"2025-05-30T02:22:59.361457Z","shell.execute_reply.started":"2025-05-30T02:22:54.732125Z","shell.execute_reply":"2025-05-30T02:22:59.360912Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a9148f7db644ff1b4a8a9e1c81a2cbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sql_create_context_v4.json:   0%|          | 0.00/21.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"199e9066d005492296a8579fd7dbbc9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/78577 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa4a3db74c0443658cfe2ea3ccff3a69"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def display_table(dataset_or_sample):\n  pd.set_option(\"display.max_colwidth\", None)\n  pd.set_option(\"display.width\", None)\n  pd.set_option(\"display.max_rows\", None)\n  if isinstance(dataset_or_sample, dict):\n      df = pd.DataFrame(dataset_or_sample, index=[0])\n  else:\n      df = pd.DataFrame(dataset_or_sample)\n  html = df.to_html().replace(\"\\n\", \"<br>\")\n  styled_html = f\"\"\"<style> .dataframe th, .dataframe tbody td {{ text-align: left; padding-right: 30px; }} </style> {html}\"\"\"\n  display(HTML(styled_html))\n\nprint(\"Displaying a few samples from the raw dataset:\")\ndisplay_table(dataset.select(range(3)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:22:59.363088Z","iopub.execute_input":"2025-05-30T02:22:59.363307Z","iopub.status.idle":"2025-05-30T02:22:59.380331Z","shell.execute_reply.started":"2025-05-30T02:22:59.363290Z","shell.execute_reply":"2025-05-30T02:22:59.379561Z"}},"outputs":[{"name":"stdout","text":"Displaying a few samples from the raw dataset:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\"><br>  <thead><br>    <tr style=\"text-align: right;\"><br>      <th></th><br>      <th>answer</th><br>      <th>question</th><br>      <th>context</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>SELECT COUNT(*) FROM head WHERE age &gt; 56</td><br>      <td>How many heads of the departments are older than 56 ?</td><br>      <td>CREATE TABLE head (age INTEGER)</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>SELECT name, born_state, age FROM head ORDER BY age</td><br>      <td>List the name, born state and age of the heads of departments ordered by age.</td><br>      <td>CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>SELECT creation, name, budget_in_billions FROM department</td><br>      <td>List the creation year, name and budget of each department.</td><br>      <td>CREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)</td><br>    </tr><br>  </tbody><br></table>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = split_dataset[\"train\"]\ntest_dataset = split_dataset[\"test\"]\n\nprint(f\"Training dataset contains {len(train_dataset)} text-to-SQL pairs\")\nprint(f\"Test dataset contains {len(test_dataset)} text-to-SQL pairs\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:22:59.381081Z","iopub.execute_input":"2025-05-30T02:22:59.381295Z","iopub.status.idle":"2025-05-30T02:22:59.408641Z","shell.execute_reply.started":"2025-05-30T02:22:59.381269Z","shell.execute_reply":"2025-05-30T02:22:59.408074Z"}},"outputs":[{"name":"stdout","text":"Training dataset contains 62861 text-to-SQL pairs\nTest dataset contains 15716 text-to-SQL pairs\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"PROMPT_TEMPLATE = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n### Table:\n{context}\n\n### Question:\n{question}\n\n### Response:\n{output}\"\"\"\n\ndef apply_prompt_template(row):\n  prompt = PROMPT_TEMPLATE.format(\n      question=row[\"question\"],\n      context=row[\"context\"],\n      output=row[\"answer\"], # During training, the 'answer' is the target output\n  )\n  return {\"prompt\": prompt}\n\ntrain_dataset_formatted = train_dataset.map(apply_prompt_template)\nprint(\"\\nDisplaying a sample from the training dataset after applying prompt template:\")\ndisplay_table(train_dataset_formatted.select(range(1)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:22:59.409380Z","iopub.execute_input":"2025-05-30T02:22:59.409562Z","iopub.status.idle":"2025-05-30T02:23:05.466298Z","shell.execute_reply.started":"2025-05-30T02:22:59.409539Z","shell.execute_reply":"2025-05-30T02:23:05.465664Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/62861 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69879fae0c624eefa93d3be24d909c81"}},"metadata":{}},{"name":"stdout","text":"\nDisplaying a sample from the training dataset after applying prompt template:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\"><br>  <thead><br>    <tr style=\"text-align: right;\"><br>      <th></th><br>      <th>answer</th><br>      <th>question</th><br>      <th>context</th><br>      <th>prompt</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>SELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"</td><br>      <td>Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?</td><br>      <td>CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)</td><br>      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\\n\\n### Table:\\nCREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)\\n\\n### Question:\\nWhich Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?\\n\\n### Response:\\nSELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"</td><br>    </tr><br>  </tbody><br></table>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"base_model_id = \"HuggingFaceTB/SmolLM2-135M\"\nMAX_LENGTH = 256 # You can adjust this\n\ntokenizer = AutoTokenizer.from_pretrained(\n  base_model_id,\n  model_max_length=MAX_LENGTH,\n  padding_side=\"left\", # Important for decoder-only models\n  add_eos_token=True,\n)\ntokenizer.pad_token = tokenizer.eos_token # Set pad token to EOS token\n\ndef tokenize_and_pad_to_fixed_length(sample):\n  result = tokenizer(\n      sample[\"prompt\"],\n      truncation=True,\n      max_length=MAX_LENGTH,\n      padding=\"max_length\", # Pad to MAX_LENGTH\n  )\n  result[\"labels\"] = result[\"input_ids\"].copy() # For Causal LM, labels are usually input_ids shifted\n  return result\n\ntokenized_train_dataset = train_dataset_formatted.map(tokenize_and_pad_to_fixed_length, batched=True) # batched=True can speed this up\n# tokenized_test_dataset = test_dataset.map(apply_prompt_template).map(tokenize_and_pad_to_fixed_length, batched=True) # Also tokenize test for eval\n\nassert all(len(x[\"input_ids\"]) == MAX_LENGTH for x in tokenized_train_dataset)\n\nprint(\"\\nDisplaying a tokenized sample from the training dataset:\")\ndisplay_table(tokenized_train_dataset.select(range(1)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:23:05.467108Z","iopub.execute_input":"2025-05-30T02:23:05.467358Z","iopub.status.idle":"2025-05-30T02:23:49.095780Z","shell.execute_reply.started":"2025-05-30T02:23:05.467340Z","shell.execute_reply":"2025-05-30T02:23:49.095143Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2c926e4e1594021878916d4b2eea802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95635c976d5244638d7f5b7251043e03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7bc5d74094a458e8981e32d7cf38e87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6dfb033067d48ee9a7249a192b07414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9f22491871940ba810be2f2df4a1b77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/62861 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"603d3528a1254498975919350100abf7"}},"metadata":{}},{"name":"stdout","text":"\nDisplaying a tokenized sample from the training dataset:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\"><br>  <thead><br>    <tr style=\"text-align: right;\"><br>      <th></th><br>      <th>answer</th><br>      <th>question</th><br>      <th>context</th><br>      <th>prompt</th><br>      <th>input_ids</th><br>      <th>attention_mask</th><br>      <th>labels</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>SELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"</td><br>      <td>Which Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?</td><br>      <td>CREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)</td><br>      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\\n\\n### Table:\\nCREATE TABLE table_name_56 (perth VARCHAR, adelaide VARCHAR, melbourne VARCHAR, gold_coast VARCHAR, sydney VARCHAR)\\n\\n### Question:\\nWhich Perth has Gold Coast yes, Sydney yes, Melbourne yes, and Adelaide yes?\\n\\n### Response:\\nSELECT perth FROM table_name_56 WHERE gold_coast = \"yes\" AND sydney = \"yes\" AND melbourne = \"yes\" AND adelaide = \"yes\"</td><br>      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td><br>      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td><br>      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td><br>    </tr><br>  </tbody><br></table>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Model Initialization","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"\\nUsing device: {device}\")\n\nquantization_config = BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_use_double_quant=True,\n  bnb_4bit_quant_type=\"nf4\",\n  bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    quantization_config=quantization_config,\n    # device_map=\"auto\" # Automatically distribute model on available GPUs if any\n)\nprint(\"\\nBase model loaded with 4-bit quantization.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:23:49.096492Z","iopub.execute_input":"2025-05-30T02:23:49.096745Z","iopub.status.idle":"2025-05-30T02:23:56.472944Z","shell.execute_reply.started":"2025-05-30T02:23:49.096722Z","shell.execute_reply":"2025-05-30T02:23:56.472322Z"}},"outputs":[{"name":"stdout","text":"\nUsing device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4d50d54062e4813a77befb74bd6d213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"689c43accee943bf96e1f3b63fb66542"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8456ca87eb4f4ea5a042298f49a816d9"}},"metadata":{}},{"name":"stdout","text":"\nBase model loaded with 4-bit quantization.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Pre-Finetuning Inference","metadata":{}},{"cell_type":"code","source":"pipeline_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\nif pipeline_tokenizer.pad_token is None:\n    pipeline_tokenizer.pad_token = pipeline_tokenizer.eos_token\n\ntext_gen_pipeline_base = hf_pipeline(\n    task=\"text-generation\",\n    model=base_model, # Use the quantized base_model\n    tokenizer=pipeline_tokenizer,\n    device_map=\"auto\"\n)\n\nsample_idx_for_test = 1\nif len(test_dataset) > sample_idx_for_test:\n    sample = test_dataset[sample_idx_for_test]\n    prompt_for_base_inference = PROMPT_TEMPLATE.format(\n      context=sample[\"context\"], question=sample[\"question\"], output=\"\" # Leave output blank for generation\n    )\n\n    print(\"\\n--- Running Inference with BASE (Quantized) Model (Before Fine-tuning) ---\")\n    with torch.no_grad():\n      response = text_gen_pipeline_base(\n          prompt_for_base_inference,\n          max_new_tokens=100, # Shorter for quick test\n          repetition_penalty=1.15,\n          return_full_text=False,\n          eos_token_id=pipeline_tokenizer.eos_token_id, # Important for stopping\n          pad_token_id=pipeline_tokenizer.pad_token_id  # Also good practice\n      )\n\n    print(\"\\nDisplaying BASE model inference result:\")\n    display_table({\"prompt\": prompt_for_base_inference, \"generated_query_base_model\": response[0][\"generated_text\"]})\nelse:\n    print(f\"\\nSkipping pre-finetuning inference as test_dataset has less than {sample_idx_for_test+1} samples.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:23:56.473657Z","iopub.execute_input":"2025-05-30T02:23:56.473946Z","iopub.status.idle":"2025-05-30T02:24:04.701112Z","shell.execute_reply.started":"2025-05-30T02:23:56.473927Z","shell.execute_reply":"2025-05-30T02:24:04.700273Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"\n--- Running Inference with BASE (Quantized) Model (Before Fine-tuning) ---\n\nDisplaying BASE model inference result:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\"><br>  <thead><br>    <tr style=\"text-align: right;\"><br>      <th></th><br>      <th>prompt</th><br>      <th>generated_query_base_model</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\\n\\n### Table:\\nCREATE TABLE table_name_61 (game INTEGER, opponent VARCHAR, record VARCHAR)\\n\\n### Question:\\nWhat is the lowest numbered game against Phoenix with a record of 29-17?\\n\\n### Response:\\n</td><br>      <td>(0 - 8). If there was no match for a player in this round, then you were out; if there wasn't an individual win/draw result or otherwise impossible result it would be reported as such after all players played their matches excepting those who had been eliminated from these games by either the opposing team or other persons playing outside of the rules which included only players on court whose results didn't influence others because they wonn'reno played some sort of exclusion rule.)\\n\\n</td><br>    </tr><br>  </tbody><br></table>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### System: You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n### Question: What is the lowest numbered game against Phoenix with a record of 29-17?\n\n### Context: CREATE TABLE table_name_61 (game INTEGER, opponent VARCHAR, record VARCHAR)\n\n### Output: If there was no match for a player in this round, then you were out; if there wasn't an individual win/draw result or otherwise impossible result it would be reported as such after all players played their matches excepting those who had been eliminated from these games by either the opposing team or other persons playing outside of the rules which included only players on court whose results didn't influence others because they wonn'reno played some sort of exclusion ","metadata":{}},{"cell_type":"markdown","source":"## Partial Fine-Tuning Setup","metadata":{}},{"cell_type":"code","source":"# --- 5.1 Prepare model for k-bit training ---\nbase_model.gradient_checkpointing_enable() # Saves memory during training\npeft_ready_model = prepare_model_for_kbit_training(base_model) # Prepares quantized model for PEFT\nprint(\"\\nModel prepared for k-bit training (QLoRA).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:24:04.703595Z","iopub.execute_input":"2025-05-30T02:24:04.704499Z","iopub.status.idle":"2025-05-30T02:24:04.716325Z","shell.execute_reply.started":"2025-05-30T02:24:04.704464Z","shell.execute_reply":"2025-05-30T02:24:04.715731Z"}},"outputs":[{"name":"stdout","text":"\nModel prepared for k-bit training (QLoRA).\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# --- 5.2 LoRA Configuration ---\npeft_config = LoraConfig(\n  task_type=\"CAUSAL_LM\",\n  r=32,\n  lora_alpha=64,\n  lora_dropout=0.1,\n  target_modules=[ # Ensure these modules exist in your model (SmolLM2 might have different names)\n      \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n      # \"gate_proj\", \"up_proj\", \"down_proj\", # These might not be in SmolLM-135M\n      # \"lm_head\", # Usually not targeted with LoRA for stability, but can be if needed.\n                  # For SmolLM2, let's check its architecture if these are appropriate.\n                  # If unsure, start with query, key, value, output projections.\n  ],\n  bias=\"none\",\n)\n\n# --- 5.3 Get PEFT Model ---\npeft_model = get_peft_model(peft_ready_model, peft_config)\nprint(\"\\nPEFT model created with LoRA adapters.\")\npeft_model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:24:04.716985Z","iopub.execute_input":"2025-05-30T02:24:04.717232Z","iopub.status.idle":"2025-05-30T02:24:05.333398Z","shell.execute_reply.started":"2025-05-30T02:24:04.717212Z","shell.execute_reply":"2025-05-30T02:24:05.332742Z"}},"outputs":[{"name":"stdout","text":"\nPEFT model created with LoRA adapters.\ntrainable params: 3,686,400 || all params: 138,201,408 || trainable%: 2.6674\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"output_dir = \"./sql_smollm2_qlora_finetuned\"\n\ntraining_args = TrainingArguments(\n  output_dir=output_dir,\n  per_device_train_batch_size=2, # Adjust based on your GPU memory\n  gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n  gradient_checkpointing=True,    # Already enabled on model, but good to have here too\n  optim=\"paged_adamw_8bit\",       # Optimizer suitable for QLoRA\n  bf16=torch.cuda.is_bf16_supported(), # Use bf16 if available, otherwise fp16 might be an alternative or disable\n  fp16=not torch.cuda.is_bf16_supported() and torch.cuda.is_available(), # Fallback to fp16 if no bf16\n  learning_rate=2e-5,\n  lr_scheduler_type=\"constant\",\n  max_steps=500,                  # Number of training steps\n  save_strategy=\"steps\",          # Save checkpoints at save_steps interval\n  save_steps=100,                 # Save a checkpoint every 100 steps\n  logging_steps=10,               # Log training metrics every 10 steps\n  warmup_steps=5,\n  ddp_find_unused_parameters=False if torch.cuda.device_count() > 1 else None, # Only for DDP\n  report_to=\"none\", # Set to \"tensorboard\", \"wandb\", etc. if you want logging\n  # run_name=f\"SmolLM2-SQL-QLoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%s')}\", # If using wandb/mlflow\n)\n\n# Data Collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n# Trainer\ntrainer = Trainer(\n  model=peft_model,\n  train_dataset=tokenized_train_dataset,\n  # eval_dataset=tokenized_test_dataset, # You would add this for evaluation during training\n  data_collator=data_collator,\n  args=training_args,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:24:05.334152Z","iopub.execute_input":"2025-05-30T02:24:05.334351Z","iopub.status.idle":"2025-05-30T02:24:05.378753Z","shell.execute_reply.started":"2025-05-30T02:24:05.334336Z","shell.execute_reply":"2025-05-30T02:24:05.378048Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Compatibility with gradient checkpointing\nif hasattr(peft_model, 'config') and hasattr(peft_model.config, 'use_cache'):\n    peft_model.config.use_cache = False\nelif hasattr(base_model, 'config') and hasattr(base_model.config, 'use_cache'): # if peft_model doesn't have it directly\n    base_model.config.use_cache = False\n\nprint(\"\\nTrainer initialized. Starting training...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:24:05.379565Z","iopub.execute_input":"2025-05-30T02:24:05.379844Z","iopub.status.idle":"2025-05-30T02:24:05.384422Z","shell.execute_reply.started":"2025-05-30T02:24:05.379819Z","shell.execute_reply":"2025-05-30T02:24:05.383691Z"}},"outputs":[{"name":"stdout","text":"\nTrainer initialized. Starting training...\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"train_result = trainer.train()\nprint(\"\\nTraining finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:24:05.385194Z","iopub.execute_input":"2025-05-30T02:24:05.385421Z","iopub.status.idle":"2025-05-30T02:39:39.665617Z","shell.execute_reply.started":"2025-05-30T02:24:05.385399Z","shell.execute_reply":"2025-05-30T02:39:39.665026Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 15:31, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.847200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.667500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.675400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.556400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.484200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.356000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.218300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.139400</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.026800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.944200</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.851400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.765200</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.651700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.495000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.437200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.355100</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.265700</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.224100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.208700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.181700</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.137200</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.112100</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.090000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.053800</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.092800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.079100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.068700</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.044200</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.054300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.015100</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.069000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.012600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.016800</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.016300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.022900</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.960600</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.958900</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.956000</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.944200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.924200</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.953000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.950700</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.944600</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.940800</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.929800</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.917400</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.940500</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.863800</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.891900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.893900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nTraining finished.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# --- Save training metrics ---\nmetrics = train_result.metrics\ntrainer.log_metrics(\"train\", metrics)\ntrainer.save_metrics(\"train\", metrics)\n\n# --- Save the fine-tuned PEFT model (adapters) ---\n# The trainer automatically saves the model to output_dir/checkpoint-xxx and the final model\n# But we can explicitly save the final adapter model as well\nfinal_adapter_dir = f\"/kaggle/working/idlyvadda\"\npeft_model.save_pretrained(final_adapter_dir)\ntokenizer.save_pretrained(final_adapter_dir) # Save tokenizer with adapter for convenience\nprint(f\"Fine-tuned PEFT adapters saved to: {final_adapter_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:39:39.666421Z","iopub.execute_input":"2025-05-30T02:39:39.666661Z","iopub.status.idle":"2025-05-30T02:39:40.294490Z","shell.execute_reply.started":"2025-05-30T02:39:39.666644Z","shell.execute_reply":"2025-05-30T02:39:40.293813Z"}},"outputs":[{"name":"stdout","text":"***** train metrics *****\n  epoch                    =     0.0636\n  total_flos               =   628794GF\n  train_loss               =     1.3641\n  train_runtime            = 0:15:33.78\n  train_samples_per_second =      4.284\n  train_steps_per_second   =      0.535\nFine-tuned PEFT adapters saved to: /kaggle/working/idlyvadda\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Post Fine-Tuning Inference","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 8: INTERACTIVE POST-FINETUNING INFERENCE (Demonstration)\n# ==============================================================================\n# This section focuses on loading your fine-tuned model and allowing you to\n# interactively provide context and questions to get SQL queries.\n\nprint(\"\\n--- SECTION 8: Interactive Post-Finetuning Inference ---\")\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline as hf_pipeline\nfrom peft import PeftModel\nimport torch\n\n# --- Configuration (Should match training/saving) ---\nbase_model_id = \"HuggingFaceTB/SmolLM2-135M\"\n# This output_dir should be where your TrainingArguments saved the final model/checkpoints\n# And where 'final_adapter' subdirectory was created.\n# Example: output_dir_from_training = \"./sql_smollm2_qlora_finetuned\"\noutput_dir_from_training = training_args.output_dir # Use the one defined in TrainingArguments\nfinal_adapter_dir = f\"/kaggle/working/idlyvadda\" # This is where we saved it\n\n# Define the prompt template again (must be identical to the one used for training)\nPROMPT_TEMPLATE = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n### Table:\n{context}\n\n### Question:\n{question}\n\n### Response:\n{output}\"\"\"\n\n# --- 8.1 Load Base Quantized Model ---\nprint(f\"\\nLoading base model '{base_model_id}' with quantization...\")\nquantization_config = BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_use_double_quant=True,\n  bnb_4bit_quant_type=\"nf4\",\n  bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n# Ensure the device is set correctly\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nloaded_base_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    quantization_config=quantization_config,\n    device_map=\"auto\" # Automatically map to device\n)\nprint(\"Base model for inference loaded.\")\n\n# --- 8.2 Load PEFT Adapters ---\nprint(f\"Loading PEFT adapters from: {final_adapter_dir}\")\n# Ensure the base model is on the correct device before loading adapters if not using device_map=\"auto\"\n# loaded_base_model.to(device)\nfinetuned_model = PeftModel.from_pretrained(loaded_base_model, final_adapter_dir)\nfinetuned_model.eval() # Set to evaluation mode\nprint(\"PEFT adapters loaded onto the base model.\")\n\n# --- 8.3 Load Tokenizer (the one saved with adapters is best) ---\nprint(f\"Loading tokenizer from: {final_adapter_dir}\")\ninference_tokenizer = AutoTokenizer.from_pretrained(final_adapter_dir)\nif inference_tokenizer.pad_token is None: # Ensure pad token is set\n    inference_tokenizer.pad_token = inference_tokenizer.eos_token\nprint(\"Tokenizer loaded.\")\n\n# --- 8.4 Create Inference Pipeline ---\ntext_gen_pipeline_finetuned = hf_pipeline(\n    task=\"text-generation\",\n    model=finetuned_model,\n    tokenizer=inference_tokenizer,\n    device_map=\"auto\" # Let pipeline handle device mapping\n)\nprint(\"Inference pipeline with fine-tuned model created.\")\n\n# --- 8.5 Interactive Function to Generate SQL ---\ndef generate_sql_query(table_context, natural_language_question, max_new_tokens=150):\n    \"\"\"\n    Generates an SQL query given table context and a natural language question\n    using the fine-tuned model.\n    \"\"\"\n    prompt = PROMPT_TEMPLATE.format(\n      context=table_context,\n      question=natural_language_question,\n      output=\"\" # Leave output blank for generation\n    )\n\n    print(\"\\n--- Generating SQL Query ---\")\n    print(f\"Table Context:\\n{table_context}\")\n    print(f\"Question:\\n{natural_language_question}\")\n\n    with torch.no_grad(): # Ensure no gradients are computed during inference\n      response = text_gen_pipeline_finetuned(\n          prompt,\n          max_new_tokens=max_new_tokens,\n          repetition_penalty=1.15, # You can tune this\n          temperature=0.7,        # You can tune this (lower for more deterministic, higher for more creative)\n          do_sample=True,         # Recommended for more natural outputs, set False for greedy\n          top_k=50,               # Consider only the top_k tokens\n          top_p=0.95,             # Nucleus sampling\n          return_full_text=False, # We only want the generated part\n          eos_token_id=inference_tokenizer.eos_token_id,\n          pad_token_id=inference_tokenizer.pad_token_id\n      )\n\n    generated_sql = response[0][\"generated_text\"].strip()\n    print(f\"\\nGenerated SQL Query:\\n{generated_sql}\")\n    return generated_sql\n\n# --- 8.6 DEMONSTRATION: Provide your custom context and question here ---\n\n# Example 1: Simple Query\nmy_table_context_1 = \"\"\"\nCREATE TABLE employees (\n    EmployeeID int,\n    FirstName varchar(255),\n    LastName varchar(255),\n    Department varchar(255),\n    Salary int\n)\n\"\"\"\nmy_question_1 = \"Show me the first names of all employees in the 'Sales' department.\"\ngenerated_sql_1 = generate_sql_query(my_table_context_1, my_question_1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:43:02.940008Z","iopub.execute_input":"2025-05-30T02:43:02.940613Z","iopub.status.idle":"2025-05-30T02:43:19.795370Z","shell.execute_reply.started":"2025-05-30T02:43:02.940589Z","shell.execute_reply":"2025-05-30T02:43:19.794722Z"}},"outputs":[{"name":"stdout","text":"\n--- SECTION 8: Interactive Post-Finetuning Inference ---\n\nLoading base model 'HuggingFaceTB/SmolLM2-135M' with quantization...\nBase model for inference loaded.\nLoading PEFT adapters from: /kaggle/working/idlyvadda\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"PEFT adapters loaded onto the base model.\nLoading tokenizer from: /kaggle/working/idlyvadda\nTokenizer loaded.\nInference pipeline with fine-tuned model created.\n\n--- Generating SQL Query ---\nTable Context:\n\nCREATE TABLE employees (\n    EmployeeID int,\n    FirstName varchar(255),\n    LastName varchar(255),\n    Department varchar(255),\n    Salary int\n)\n\nQuestion:\nShow me the first names of all employees in the 'Sales' department.\n\nGenerated SQL Query:\nSELECT FIRSTNAME AS FIRSTNAME FROM employees WHERE DEPENDENT = \"sales\" AND DEPT = 4903879601_DURATION - 2665846799599708699599707960579198618290873254676598365195309651764384389491959970764836337666316068899419889551\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Example 2: A slightly more complex query\nmy_table_context_2 = \"\"\"\nCREATE TABLE products (\n    ProductID int,\n    ProductName varchar(255),\n    CategoryID int,\n    Price decimal\n);\nCREATE TABLE categories (\n    CategoryID int,\n    CategoryName varchar(255)\n)\n\"\"\"\nmy_question_2 = \"What are the names of products that belong to the 'Electronics' category and cost more than 500?\"\ngenerated_sql_2 = generate_sql_query(my_table_context_2, my_question_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:43:19.796568Z","iopub.execute_input":"2025-05-30T02:43:19.796795Z","iopub.status.idle":"2025-05-30T02:43:33.299259Z","shell.execute_reply.started":"2025-05-30T02:43:19.796778Z","shell.execute_reply":"2025-05-30T02:43:33.298597Z"}},"outputs":[{"name":"stdout","text":"\n--- Generating SQL Query ---\nTable Context:\n\nCREATE TABLE products (\n    ProductID int,\n    ProductName varchar(255),\n    CategoryID int,\n    Price decimal\n);\nCREATE TABLE categories (\n    CategoryID int,\n    CategoryName varchar(255)\n)\n\nQuestion:\nWhat are the names of products that belong to the 'Electronics' category and cost more than 500?\n\nGenerated SQL Query:\nSELECT TOP 3 productname FROM products WHERE categoryid > 14 AND price < 879 ORDER BY name ASC;\n\n\n### Output:\n SELECT TOP 3 productname FROM products WHERE categoryid > 14 AND price < 879 ORDER BY name ASC;\n\n\n### Solution:\nSELECT TOP 3 productname FROM products WHERE categoryid > 14 AND price < 649 ORDER BY name ASC;\n\n\n```sql\nSELECT TOP 3 productname FROM products WHERE categoryid > 14 AND price < 879 ORDER BY name ASC;\n\n\n```\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}