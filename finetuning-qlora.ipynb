{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment Setup and Imports","metadata":{}},{"cell_type":"code","source":"!pip install -q -U transformers peft datasets accelerate trl bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:26:42.056570Z","iopub.execute_input":"2025-05-29T18:26:42.056809Z","iopub.status.idle":"2025-05-29T18:28:04.487061Z","shell.execute_reply.started":"2025-05-29T18:26:42.056784Z","shell.execute_reply":"2025-05-29T18:28:04.486082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nimport pandas as pd\nfrom datasets import load_dataset\nfrom IPython.display import HTML, display\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline as hf_pipeline, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:28:04.489085Z","iopub.execute_input":"2025-05-29T18:28:04.489334Z","iopub.status.idle":"2025-05-29T18:28:30.173838Z","shell.execute_reply.started":"2025-05-29T18:28:04.489310Z","shell.execute_reply":"2025-05-29T18:28:30.173132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Preparation","metadata":{}},{"cell_type":"code","source":"# --- 2.1 Load Raw Dataset ---\ndataset_name = \"b-mc2/sql-create-context\"\ndataset = load_dataset(dataset_name, split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:28:30.174471Z","iopub.execute_input":"2025-05-29T18:28:30.174976Z","iopub.status.idle":"2025-05-29T18:28:32.394841Z","shell.execute_reply.started":"2025-05-29T18:28:30.174958Z","shell.execute_reply":"2025-05-29T18:28:32.394218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_table(dataset_or_sample):\n  pd.set_option(\"display.max_colwidth\", None)\n  pd.set_option(\"display.width\", None)\n  pd.set_option(\"display.max_rows\", None)\n  if isinstance(dataset_or_sample, dict):\n      df = pd.DataFrame(dataset_or_sample, index=[0])\n  else:\n      df = pd.DataFrame(dataset_or_sample)\n  html = df.to_html().replace(\"\\n\", \"<br>\")\n  styled_html = f\"\"\"<style> .dataframe th, .dataframe tbody td {{ text-align: left; padding-right: 30px; }} </style> {html}\"\"\"\n  display(HTML(styled_html))\n\nprint(\"Displaying a few samples from the raw dataset:\")\ndisplay_table(dataset.select(range(3)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:28:32.395612Z","iopub.execute_input":"2025-05-29T18:28:32.395870Z","iopub.status.idle":"2025-05-29T18:28:32.420414Z","shell.execute_reply.started":"2025-05-29T18:28:32.395844Z","shell.execute_reply":"2025-05-29T18:28:32.419501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = split_dataset[\"train\"]\ntest_dataset = split_dataset[\"test\"]\n\nprint(f\"Training dataset contains {len(train_dataset)} text-to-SQL pairs\")\nprint(f\"Test dataset contains {len(test_dataset)} text-to-SQL pairs\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:28:32.421631Z","iopub.execute_input":"2025-05-29T18:28:32.422279Z","iopub.status.idle":"2025-05-29T18:28:34.553089Z","shell.execute_reply.started":"2025-05-29T18:28:32.422246Z","shell.execute_reply":"2025-05-29T18:28:34.552491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PROMPT_TEMPLATE = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n### Table:\n{context}\n\n### Question:\n{question}\n\n### Response:\n{output}\"\"\"\n\ndef apply_prompt_template(row):\n  prompt = PROMPT_TEMPLATE.format(\n      question=row[\"question\"],\n      context=row[\"context\"],\n      output=row[\"answer\"], # During training, the 'answer' is the target output\n  )\n  return {\"prompt\": prompt}\n\ntrain_dataset_formatted = train_dataset.map(apply_prompt_template)\nprint(\"\\nDisplaying a sample from the training dataset after applying prompt template:\")\ndisplay_table(train_dataset_formatted.select(range(1)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:28:34.553868Z","iopub.execute_input":"2025-05-29T18:28:34.554074Z","iopub.status.idle":"2025-05-29T18:28:40.497312Z","shell.execute_reply.started":"2025-05-29T18:28:34.554055Z","shell.execute_reply":"2025-05-29T18:28:40.496579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_model_id = \"HuggingFaceTB/SmolLM2-135M\"\nMAX_LENGTH = 256 # You can adjust this\n\ntokenizer = AutoTokenizer.from_pretrained(\n  base_model_id,\n  model_max_length=MAX_LENGTH,\n  padding_side=\"left\", # Important for decoder-only models\n  add_eos_token=True,\n)\ntokenizer.pad_token = tokenizer.eos_token # Set pad token to EOS token\n\ndef tokenize_and_pad_to_fixed_length(sample):\n  result = tokenizer(\n      sample[\"prompt\"],\n      truncation=True,\n      max_length=MAX_LENGTH,\n      padding=\"max_length\", # Pad to MAX_LENGTH\n  )\n  result[\"labels\"] = result[\"input_ids\"].copy() # For Causal LM, labels are usually input_ids shifted\n  return result\n\ntokenized_train_dataset = train_dataset_formatted.map(tokenize_and_pad_to_fixed_length, batched=True) # batched=True can speed this up\n# tokenized_test_dataset = test_dataset.map(apply_prompt_template).map(tokenize_and_pad_to_fixed_length, batched=True) # Also tokenize test for eval\n\nassert all(len(x[\"input_ids\"]) == MAX_LENGTH for x in tokenized_train_dataset)\n\nprint(\"\\nDisplaying a tokenized sample from the training dataset:\")\ndisplay_table(tokenized_train_dataset.select(range(1)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:28:40.499606Z","iopub.execute_input":"2025-05-29T18:28:40.499838Z","iopub.status.idle":"2025-05-29T18:29:22.284376Z","shell.execute_reply.started":"2025-05-29T18:28:40.499819Z","shell.execute_reply":"2025-05-29T18:29:22.283612Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Initialization","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"\\nUsing device: {device}\")\n\nquantization_config = BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_use_double_quant=True,\n  bnb_4bit_quant_type=\"nf4\",\n  bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    quantization_config=quantization_config,\n    # device_map=\"auto\" # Automatically distribute model on available GPUs if any\n)\nprint(\"\\nBase model loaded with 4-bit quantization.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:29:22.285548Z","iopub.execute_input":"2025-05-29T18:29:22.285836Z","iopub.status.idle":"2025-05-29T18:29:29.742404Z","shell.execute_reply.started":"2025-05-29T18:29:22.285803Z","shell.execute_reply":"2025-05-29T18:29:29.741791Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pre-Finetuning Inference","metadata":{}},{"cell_type":"code","source":"pipeline_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\nif pipeline_tokenizer.pad_token is None:\n    pipeline_tokenizer.pad_token = pipeline_tokenizer.eos_token\n\ntext_gen_pipeline_base = hf_pipeline(\n    task=\"text-generation\",\n    model=base_model, # Use the quantized base_model\n    tokenizer=pipeline_tokenizer,\n    device_map=\"auto\"\n)\n\nsample_idx_for_test = 1\nif len(test_dataset) > sample_idx_for_test:\n    sample = test_dataset[sample_idx_for_test]\n    prompt_for_base_inference = PROMPT_TEMPLATE.format(\n      context=sample[\"context\"], question=sample[\"question\"], output=\"\" # Leave output blank for generation\n    )\n\n    print(\"\\n--- Running Inference with BASE (Quantized) Model (Before Fine-tuning) ---\")\n    with torch.no_grad():\n      response = text_gen_pipeline_base(\n          prompt_for_base_inference,\n          max_new_tokens=100, # Shorter for quick test\n          repetition_penalty=1.15,\n          return_full_text=False,\n          eos_token_id=pipeline_tokenizer.eos_token_id, # Important for stopping\n          pad_token_id=pipeline_tokenizer.pad_token_id  # Also good practice\n      )\n\n    print(\"\\nDisplaying BASE model inference result:\")\n    display_table({\"prompt\": prompt_for_base_inference, \"generated_query_base_model\": response[0][\"generated_text\"]})\nelse:\n    print(f\"\\nSkipping pre-finetuning inference as test_dataset has less than {sample_idx_for_test+1} samples.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:29:29.743236Z","iopub.execute_input":"2025-05-29T18:29:29.743968Z","iopub.status.idle":"2025-05-29T18:29:35.810427Z","shell.execute_reply.started":"2025-05-29T18:29:29.743949Z","shell.execute_reply":"2025-05-29T18:29:35.809972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### System: You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n### Question: What is the lowest numbered game against Phoenix with a record of 29-17?\n\n### Context: CREATE TABLE table_name_61 (game INTEGER, opponent VARCHAR, record VARCHAR)","metadata":{}},{"cell_type":"markdown","source":"## Partial Fine-Tuning Setup","metadata":{}},{"cell_type":"code","source":"# --- 5.1 Prepare model for k-bit training ---\nbase_model.gradient_checkpointing_enable() # Saves memory during training\npeft_ready_model = prepare_model_for_kbit_training(base_model) # Prepares quantized model for PEFT\nprint(\"\\nModel prepared for k-bit training (QLoRA).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:29:35.816154Z","iopub.execute_input":"2025-05-29T18:29:35.816477Z","iopub.status.idle":"2025-05-29T18:29:35.835905Z","shell.execute_reply.started":"2025-05-29T18:29:35.816433Z","shell.execute_reply":"2025-05-29T18:29:35.834767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5.2 LoRA Configuration ---\npeft_config = LoraConfig(\n  task_type=\"CAUSAL_LM\",\n  r=32,\n  lora_alpha=64,\n  lora_dropout=0.1,\n  target_modules=[ # Ensure these modules exist in your model (SmolLM2 might have different names)\n      \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n      # \"gate_proj\", \"up_proj\", \"down_proj\", # These might not be in SmolLM-135M\n      # \"lm_head\", # Usually not targeted with LoRA for stability, but can be if needed.\n                  # For SmolLM2, let's check its architecture if these are appropriate.\n                  # If unsure, start with query, key, value, output projections.\n  ],\n  bias=\"none\",\n)\n\n# --- 5.3 Get PEFT Model ---\npeft_model = get_peft_model(peft_ready_model, peft_config)\nprint(\"\\nPEFT model created with LoRA adapters.\")\npeft_model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:29:35.836978Z","iopub.execute_input":"2025-05-29T18:29:35.837286Z","iopub.status.idle":"2025-05-29T18:29:36.029575Z","shell.execute_reply.started":"2025-05-29T18:29:35.837263Z","shell.execute_reply":"2025-05-29T18:29:36.028935Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"output_dir = \"./sql_smollm2_qlora_finetuned\"\n\ntraining_args = TrainingArguments(\n  output_dir=output_dir,\n  per_device_train_batch_size=2, # Adjust based on your GPU memory\n  gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n  gradient_checkpointing=True,    # Already enabled on model, but good to have here too\n  optim=\"paged_adamw_8bit\",       # Optimizer suitable for QLoRA\n  bf16=torch.cuda.is_bf16_supported(), # Use bf16 if available, otherwise fp16 might be an alternative or disable\n  fp16=not torch.cuda.is_bf16_supported() and torch.cuda.is_available(), # Fallback to fp16 if no bf16\n  learning_rate=2e-5,\n  lr_scheduler_type=\"constant\",\n  max_steps=500,                  # Number of training steps\n  save_strategy=\"steps\",          # Save checkpoints at save_steps interval\n  save_steps=100,                 # Save a checkpoint every 100 steps\n  logging_steps=10,               # Log training metrics every 10 steps\n  warmup_steps=5,\n  ddp_find_unused_parameters=False if torch.cuda.device_count() > 1 else None, # Only for DDP\n  report_to=\"none\", # Set to \"tensorboard\", \"wandb\", etc. if you want logging\n  # run_name=f\"SmolLM2-SQL-QLoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%s')}\", # If using wandb/mlflow\n)\n\n# Data Collator\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n# Trainer\ntrainer = Trainer(\n  model=peft_model,\n  train_dataset=tokenized_train_dataset,\n  # eval_dataset=tokenized_test_dataset, # You would add this for evaluation during training\n  data_collator=data_collator,\n  args=training_args,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:29:56.850328Z","iopub.execute_input":"2025-05-29T18:29:56.851124Z","iopub.status.idle":"2025-05-29T18:29:56.895471Z","shell.execute_reply.started":"2025-05-29T18:29:56.851100Z","shell.execute_reply":"2025-05-29T18:29:56.894798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compatibility with gradient checkpointing\nif hasattr(peft_model, 'config') and hasattr(peft_model.config, 'use_cache'):\n    peft_model.config.use_cache = False\nelif hasattr(base_model, 'config') and hasattr(base_model.config, 'use_cache'): # if peft_model doesn't have it directly\n    base_model.config.use_cache = False\n\nprint(\"\\nTrainer initialized. Starting training...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:30:09.864998Z","iopub.execute_input":"2025-05-29T18:30:09.865662Z","iopub.status.idle":"2025-05-29T18:30:09.870238Z","shell.execute_reply.started":"2025-05-29T18:30:09.865640Z","shell.execute_reply":"2025-05-29T18:30:09.869510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_result = trainer.train()\nprint(\"\\nTraining finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:30:20.748234Z","iopub.execute_input":"2025-05-29T18:30:20.748971Z","iopub.status.idle":"2025-05-29T18:45:50.539490Z","shell.execute_reply.started":"2025-05-29T18:30:20.748947Z","shell.execute_reply":"2025-05-29T18:45:50.538873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Save training metrics ---\nmetrics = train_result.metrics\ntrainer.log_metrics(\"train\", metrics)\ntrainer.save_metrics(\"train\", metrics)\n\n# --- Save the fine-tuned PEFT model (adapters) ---\n# The trainer automatically saves the model to output_dir/checkpoint-xxx and the final model\n# But we can explicitly save the final adapter model as well\nfinal_adapter_dir = f\"/kaggle/working/idlyvadda\"\npeft_model.save_pretrained(final_adapter_dir)\ntokenizer.save_pretrained(final_adapter_dir) # Save tokenizer with adapter for convenience\nprint(f\"Fine-tuned PEFT adapters saved to: {final_adapter_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:50:53.132987Z","iopub.execute_input":"2025-05-29T18:50:53.133257Z","iopub.status.idle":"2025-05-29T18:50:53.648175Z","shell.execute_reply.started":"2025-05-29T18:50:53.133236Z","shell.execute_reply":"2025-05-29T18:50:53.647378Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Post Fine-Tuning Inference","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# SECTION 8: INTERACTIVE POST-FINETUNING INFERENCE (Demonstration)\n# ==============================================================================\n# This section focuses on loading your fine-tuned model and allowing you to\n# interactively provide context and questions to get SQL queries.\n\nprint(\"\\n--- SECTION 8: Interactive Post-Finetuning Inference ---\")\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline as hf_pipeline\nfrom peft import PeftModel\nimport torch\n\n# --- Configuration (Should match training/saving) ---\nbase_model_id = \"HuggingFaceTB/SmolLM2-135M\"\n# This output_dir should be where your TrainingArguments saved the final model/checkpoints\n# And where 'final_adapter' subdirectory was created.\n# Example: output_dir_from_training = \"./sql_smollm2_qlora_finetuned\"\noutput_dir_from_training = training_args.output_dir # Use the one defined in TrainingArguments\nfinal_adapter_dir = f\"{output_dir_from_training}/final_adapter\" # This is where we saved it\n\n# Define the prompt template again (must be identical to the one used for training)\nPROMPT_TEMPLATE = \"\"\"You are a powerful text-to-SQL model. Given the SQL tables and natural language question, your job is to write SQL query that answers the question.\n\n### Table:\n{context}\n\n### Question:\n{question}\n\n### Response:\n{output}\"\"\"\n\n# --- 8.1 Load Base Quantized Model ---\nprint(f\"\\nLoading base model '{base_model_id}' with quantization...\")\nquantization_config = BitsAndBytesConfig(\n  load_in_4bit=True,\n  bnb_4bit_use_double_quant=True,\n  bnb_4bit_quant_type=\"nf4\",\n  bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n# Ensure the device is set correctly\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nloaded_base_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    quantization_config=quantization_config,\n    device_map=\"auto\" # Automatically map to device\n)\nprint(\"Base model for inference loaded.\")\n\n# --- 8.2 Load PEFT Adapters ---\nprint(f\"Loading PEFT adapters from: {final_adapter_dir}\")\n# Ensure the base model is on the correct device before loading adapters if not using device_map=\"auto\"\n# loaded_base_model.to(device)\nfinetuned_model = PeftModel.from_pretrained(loaded_base_model, final_adapter_dir)\nfinetuned_model.eval() # Set to evaluation mode\nprint(\"PEFT adapters loaded onto the base model.\")\n\n# --- 8.3 Load Tokenizer (the one saved with adapters is best) ---\nprint(f\"Loading tokenizer from: {final_adapter_dir}\")\ninference_tokenizer = AutoTokenizer.from_pretrained(final_adapter_dir)\nif inference_tokenizer.pad_token is None: # Ensure pad token is set\n    inference_tokenizer.pad_token = inference_tokenizer.eos_token\nprint(\"Tokenizer loaded.\")\n\n# --- 8.4 Create Inference Pipeline ---\ntext_gen_pipeline_finetuned = hf_pipeline(\n    task=\"text-generation\",\n    model=finetuned_model,\n    tokenizer=inference_tokenizer,\n    device_map=\"auto\" # Let pipeline handle device mapping\n)\nprint(\"Inference pipeline with fine-tuned model created.\")\n\n# --- 8.5 Interactive Function to Generate SQL ---\ndef generate_sql_query(table_context, natural_language_question, max_new_tokens=150):\n    \"\"\"\n    Generates an SQL query given table context and a natural language question\n    using the fine-tuned model.\n    \"\"\"\n    prompt = PROMPT_TEMPLATE.format(\n      context=table_context,\n      question=natural_language_question,\n      output=\"\" # Leave output blank for generation\n    )\n\n    print(\"\\n--- Generating SQL Query ---\")\n    print(f\"Table Context:\\n{table_context}\")\n    print(f\"Question:\\n{natural_language_question}\")\n\n    with torch.no_grad(): # Ensure no gradients are computed during inference\n      response = text_gen_pipeline_finetuned(\n          prompt,\n          max_new_tokens=max_new_tokens,\n          repetition_penalty=1.15, # You can tune this\n          temperature=0.7,        # You can tune this (lower for more deterministic, higher for more creative)\n          do_sample=True,         # Recommended for more natural outputs, set False for greedy\n          top_k=50,               # Consider only the top_k tokens\n          top_p=0.95,             # Nucleus sampling\n          return_full_text=False, # We only want the generated part\n          eos_token_id=inference_tokenizer.eos_token_id,\n          pad_token_id=inference_tokenizer.pad_token_id\n      )\n\n    generated_sql = response[0][\"generated_text\"].strip()\n    print(f\"\\nGenerated SQL Query:\\n{generated_sql}\")\n    return generated_sql\n\n# --- 8.6 DEMONSTRATION: Provide your custom context and question here ---\n\n# Example 1: Simple Query\nmy_table_context_1 = \"\"\"\nCREATE TABLE employees (\n    EmployeeID int,\n    FirstName varchar(255),\n    LastName varchar(255),\n    Department varchar(255),\n    Salary int\n)\n\"\"\"\nmy_question_1 = \"Show me the first names of all employees in the 'Sales' department.\"\ngenerated_sql_1 = generate_sql_query(my_table_context_1, my_question_1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:46:23.415029Z","iopub.execute_input":"2025-05-29T18:46:23.415276Z","iopub.status.idle":"2025-05-29T18:46:39.447289Z","shell.execute_reply.started":"2025-05-29T18:46:23.415250Z","shell.execute_reply":"2025-05-29T18:46:39.446660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example 2: A slightly more complex query\nmy_table_context_2 = \"\"\"\nCREATE TABLE products (\n    ProductID int,\n    ProductName varchar(255),\n    CategoryID int,\n    Price decimal\n);\nCREATE TABLE categories (\n    CategoryID int,\n    CategoryName varchar(255)\n)\n\"\"\"\nmy_question_2 = \"What are the names of products that belong to the 'Electronics' category and cost more than 500?\"\ngenerated_sql_2 = generate_sql_query(my_table_context_2, my_question_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T18:46:39.447997Z","iopub.execute_input":"2025-05-29T18:46:39.448199Z","iopub.status.idle":"2025-05-29T18:46:54.224603Z","shell.execute_reply.started":"2025-05-29T18:46:39.448175Z","shell.execute_reply":"2025-05-29T18:46:54.223842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}